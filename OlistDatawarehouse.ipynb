{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OlistDatawarehouse.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T6E1q8QkZvC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dde8cf05-ac4e-4693-e527-d980a38c9332"
      },
      "source": [
        "!pip install SQLAlchemy\n",
        "!apt install unixodbc-dev\n",
        "!pip install pyodbc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: SQLAlchemy in /usr/local/lib/python3.6/dist-packages (1.3.20)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  unixodbc-dev\n",
            "0 upgraded, 1 newly installed, 0 to remove and 14 not upgraded.\n",
            "Need to get 217 kB of archives.\n",
            "After this operation, 1,779 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 unixodbc-dev amd64 2.3.4-1.1ubuntu3 [217 kB]\n",
            "Fetched 217 kB in 1s (396 kB/s)\n",
            "Selecting previously unselected package unixodbc-dev:amd64.\n",
            "(Reading database ... 144865 files and directories currently installed.)\n",
            "Preparing to unpack .../unixodbc-dev_2.3.4-1.1ubuntu3_amd64.deb ...\n",
            "Unpacking unixodbc-dev:amd64 (2.3.4-1.1ubuntu3) ...\n",
            "Setting up unixodbc-dev:amd64 (2.3.4-1.1ubuntu3) ...\n",
            "Collecting pyodbc\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/0d/bb08bb16c97765244791c73e49de9fd4c24bb3ef00313aed82e5640dee5d/pyodbc-4.0.30.tar.gz (266kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 5.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyodbc\n",
            "  Building wheel for pyodbc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyodbc: filename=pyodbc-4.0.30-cp36-cp36m-linux_x86_64.whl size=272678 sha256=d79b14a06aa17f9d0433870da430ff3b8b38d3c71257e4e5e477bb10c4a431ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/02/ba/6b495fec7cb127583a769a74f3ba91eb700d73e25fbb5ba09b\n",
            "Successfully built pyodbc\n",
            "Installing collected packages: pyodbc\n",
            "Successfully installed pyodbc-4.0.30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VDuKNJVkxKd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b544083-2dfb-475d-e535-dc1cd7b75ad6"
      },
      "source": [
        "%%sh\n",
        "curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add -\n",
        "curl https://packages.microsoft.com/config/ubuntu/16.04/prod.list > /etc/apt/sources.list.d/mssql-release.list\n",
        "sudo apt-get update\n",
        "sudo ACCEPT_EULA=Y apt-get -q -y install msodbcsql17"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OK\n",
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:2 https://packages.microsoft.com/ubuntu/16.04/prod xenial InRelease [4,003 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:8 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Get:9 https://packages.microsoft.com/ubuntu/16.04/prod xenial/main amd64 Packages [193 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:11 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [237 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [15.3 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,372 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,814 kB]\n",
            "Get:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:20 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [40.7 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,136 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,699 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [53.8 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [266 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,244 kB]\n",
            "Get:26 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [870 kB]\n",
            "Get:27 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [46.5 kB]\n",
            "Fetched 11.3 MB in 3s (4,032 kB/s)\n",
            "Reading package lists...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  libodbc1 odbcinst odbcinst1debian2 unixodbc unixodbc-dev\n",
            "Suggested packages:\n",
            "  unixodbc-bin\n",
            "The following NEW packages will be installed:\n",
            "  msodbcsql17 unixodbc\n",
            "The following packages will be upgraded:\n",
            "  libodbc1 odbcinst odbcinst1debian2 unixodbc-dev\n",
            "4 upgraded, 2 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 1,457 kB of archives.\n",
            "After this operation, 152 kB of additional disk space will be used.\n",
            "Get:1 https://packages.microsoft.com/ubuntu/16.04/prod xenial/main amd64 odbcinst amd64 2.3.7 [12.0 kB]\n",
            "Get:2 https://packages.microsoft.com/ubuntu/16.04/prod xenial/main amd64 unixodbc-dev amd64 2.3.7 [37.1 kB]\n",
            "Get:3 https://packages.microsoft.com/ubuntu/16.04/prod xenial/main amd64 odbcinst1debian2 amd64 2.3.7 [135 kB]\n",
            "Get:4 https://packages.microsoft.com/ubuntu/16.04/prod xenial/main amd64 libodbc1 amd64 2.3.7 [511 kB]\n",
            "Get:5 https://packages.microsoft.com/ubuntu/16.04/prod xenial/main amd64 unixodbc amd64 2.3.7 [19.6 kB]\n",
            "Get:6 https://packages.microsoft.com/ubuntu/16.04/prod xenial/main amd64 msodbcsql17 amd64 17.6.1.1-1 [743 kB]\n",
            "Fetched 1,457 kB in 0s (7,811 kB/s)\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 144886 files and directories currently installed.)\r\n",
            "Preparing to unpack .../0-odbcinst_2.3.7_amd64.deb ...\r\n",
            "Unpacking odbcinst (2.3.7) over (2.3.4-1.1ubuntu3) ...\r\n",
            "Preparing to unpack .../1-unixodbc-dev_2.3.7_amd64.deb ...\r\n",
            "Unpacking unixodbc-dev (2.3.7) over (2.3.4-1.1ubuntu3) ...\r\n",
            "Preparing to unpack .../2-odbcinst1debian2_2.3.7_amd64.deb ...\r\n",
            "Unpacking odbcinst1debian2:amd64 (2.3.7) over (2.3.4-1.1ubuntu3) ...\r\n",
            "Preparing to unpack .../3-libodbc1_2.3.7_amd64.deb ...\r\n",
            "Unpacking libodbc1:amd64 (2.3.7) over (2.3.4-1.1ubuntu3) ...\r\n",
            "Selecting previously unselected package unixodbc.\r\n",
            "Preparing to unpack .../4-unixodbc_2.3.7_amd64.deb ...\r\n",
            "Unpacking unixodbc (2.3.7) ...\r\n",
            "Selecting previously unselected package msodbcsql17.\r\n",
            "Preparing to unpack .../5-msodbcsql17_17.6.1.1-1_amd64.deb ...\r\n",
            "debconf: unable to initialize frontend: Dialog\r\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\r\n",
            "debconf: falling back to frontend: Readline\r\n",
            "Unpacking msodbcsql17 (17.6.1.1-1) ...\r\n",
            "Setting up libodbc1:amd64 (2.3.7) ...\r\n",
            "Setting up odbcinst1debian2:amd64 (2.3.7) ...\r\n",
            "Setting up odbcinst (2.3.7) ...\r\n",
            "Setting up unixodbc (2.3.7) ...\r\n",
            "Setting up unixodbc-dev (2.3.7) ...\r\n",
            "Setting up msodbcsql17 (17.6.1.1-1) ...\r\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\r\n",
            "\r\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\r\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Warning: apt-key output should not be parsed (stdout is not a terminal)\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   983  100   983    0     0   7071      0 --:--:-- --:--:-- --:--:--  7021\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100    79  100    79    0     0    929      0 --:--:-- --:--:-- --:--:--   940\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 6.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqkntRJlke_N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0988a59-fbfc-4b5e-f208-761f830af600"
      },
      "source": [
        "import pyodbc \n",
        "\n",
        "pyodbc.drivers()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ODBC Driver 17 for SQL Server']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85zNSdc3twc3"
      },
      "source": [
        "import json\r\n",
        "\r\n",
        "# carregando arquivo com as chaves para o banco de dados\r\n",
        "credentials = open('mssql_olist_data_keys.json')\r\n",
        "credentials = json.load(credentials)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehLu1cjQ4q8J"
      },
      "source": [
        "try:\n",
        "  conexion = pyodbc.connect(DRIVER = '{ODBC Driver 17 for SQL Server}',\n",
        "                            SERVER = credentials['mssql_server'],\n",
        "                            DATABASE = credentials['mssql_database'],\n",
        "                            UID = credentials['mssql_uid'],\n",
        "                            PWD = credentials['mssql_pwd'])\n",
        "  \n",
        "\n",
        "  cursor = conexion.cursor()\n",
        "\n",
        "\n",
        "  cursor.execute(\n",
        "      \"\"\"\n",
        "      IF NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = N'customers_dataset')\n",
        "      BEGIN\n",
        "        CREATE TABLE customers_dataset (\n",
        "          id_num INT NOT NULL IDENTITY (1, 1) PRIMARY KEY,\n",
        "          customer_id VARCHAR(100),\n",
        "          customer_unique_id VARCHAR(100),\n",
        "          customer_zip_code_prefix int,\n",
        "          customer_city varchar(100),\n",
        "          customer_state varchar(100)\n",
        "      )\n",
        "      END;\n",
        "      \"\"\"\n",
        "  )\n",
        "\n",
        "\n",
        "  cursor.execute(\n",
        "      \"\"\"\n",
        "      IF NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = N'geolocation_dataset')\n",
        "      BEGIN\n",
        "        CREATE TABLE geolocation_dataset (\n",
        "          id_num INT NOT NULL IDENTITY (1, 1) PRIMARY KEY,\n",
        "          geolocation_zip_code_prefix INT,\n",
        "          geolocation_lat FLOAT,\n",
        "          geolocation_lng FLOAT,\n",
        "          geolocation_city VARCHAR(100),\n",
        "          geolocation_state VARCHAR(100)\n",
        "      )\n",
        "      END;\n",
        "      \"\"\"\n",
        "  )\n",
        "\n",
        "\n",
        "  cursor.execute(\n",
        "      \"\"\"\n",
        "      IF NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = N'order_items_dataset')\n",
        "      BEGIN\n",
        "        CREATE TABLE order_items_dataset (\n",
        "          id_num INT NOT NULL IDENTITY (1, 1) PRIMARY KEY,\n",
        "          order_id VARCHAR(100),\n",
        "          order_item_id INT,\n",
        "          product_id VARCHAR(100),\n",
        "          seller_id VARCHAR(100),\n",
        "          shipping_limit_date DATETIME,\n",
        "          price FLOAT,\n",
        "          freight_value FLOAT\n",
        "      )\n",
        "      END;\n",
        "      \"\"\"\n",
        "  )\n",
        "\n",
        "\n",
        "  cursor.execute(\n",
        "      \"\"\"\n",
        "      IF NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = N'order_payments_dataset')\n",
        "      BEGIN\n",
        "        CREATE TABLE order_payments_dataset (\n",
        "          id_num INT NOT NULL IDENTITY (1, 1) PRIMARY KEY,\n",
        "          order_id VARCHAR(100),\n",
        "          payment_sequential INT,\n",
        "          payment_type VARCHAR(100),\n",
        "          payment_installments INT,\n",
        "          payment_value FLOAT\n",
        "      )\n",
        "      END;\n",
        "      \"\"\"\n",
        "  )\n",
        "\n",
        "\n",
        "  cursor.execute(\n",
        "      \"\"\"\n",
        "      IF NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = N'order_reviews_dataset')\n",
        "      BEGIN\n",
        "        CREATE TABLE order_reviews_dataset (\n",
        "          id_num INT NOT NULL IDENTITY (1, 1) PRIMARY KEY,\n",
        "          review_id VARCHAR(100),\n",
        "          order_id VARCHAR(100),\n",
        "          review_score INT,\n",
        "          review_comment_title VARCHAR(100),\n",
        "          review_comment_message VARCHAR(100),\n",
        "          review_creation_date DATETIME,\n",
        "          review_answer_timestamp DATETIME        \n",
        "      )\n",
        "      END;\n",
        "      \"\"\"\n",
        "  )\n",
        "\n",
        "\n",
        "  cursor.execute(\n",
        "      \"\"\"\n",
        "      IF NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = N'orders_dataset')\n",
        "      BEGIN\n",
        "        CREATE TABLE orders_dataset (\n",
        "          id_num INT NOT NULL IDENTITY (1, 1) PRIMARY KEY,\n",
        "          order_id VARCHAR(100),\n",
        "          customer_id VARCHAR(100),\n",
        "          order_status VARCHAR(100),\n",
        "          order_purchase_timestamp DATETIME,\n",
        "          order_approved_at DATETIME,\n",
        "          order_delivered_at DATETIME,\n",
        "          order_delivered_carrier_date DATETIME,\n",
        "          order_delivered_customer_date DATETIME,\n",
        "          order_estimated_delivery_date DATETIME\n",
        "      )\n",
        "      END;\n",
        "      \"\"\"\n",
        "  )\n",
        "\n",
        "\n",
        "  cursor.execute(\n",
        "      \"\"\"\n",
        "      IF NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = N'products_dataset')\n",
        "      BEGIN\n",
        "        CREATE TABLE products_dataset (\n",
        "          id_num INT NOT NULL IDENTITY (1, 1) PRIMARY KEY,\n",
        "          product_id VARCHAR(100),\n",
        "          product_category_name VARCHAR(100),\n",
        "          product_name_lenght FLOAT,\n",
        "          product_description_lenght FLOAT,\n",
        "          products_photos_qty FLOAT,\n",
        "          product_weight_g FLOAT,\n",
        "          product_lenght_cm FLOAT,\n",
        "          product_height_cm FLOAT,\n",
        "          product_width_cm FLOAT\n",
        "      )\n",
        "      END;\n",
        "      \"\"\"\n",
        "  )\n",
        "\n",
        "\n",
        "  cursor.execute(\n",
        "      \"\"\"\n",
        "      IF NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = N'sellers_dataset')\n",
        "      BEGIN\n",
        "        CREATE TABLE sellers_dataset (\n",
        "          id_num INT NOT NULL IDENTITY (1, 1) PRIMARY KEY,\n",
        "          seller_id VARCHAR(100),\n",
        "          seller_zip_code_prefix INT,\n",
        "          seller_city VARCHAR(100),\n",
        "          seller_state VARCHAR(100)\n",
        "      )\n",
        "      END; \n",
        "      \"\"\"\n",
        "  )\n",
        "\n",
        "\n",
        "  cursor.execute(\n",
        "      \"\"\"\n",
        "      IF NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = N'product_category_name_translation')\n",
        "      BEGIN\n",
        "        CREATE TABLE product_category_name_translation (\n",
        "          id_num INT NOT NULL IDENTITY (1, 1) PRIMARY KEY,\n",
        "          product_category_name VARCHAR(100),\n",
        "          product_category_name_english VARCHAR(100)\n",
        "      )\n",
        "      END; \n",
        "      \"\"\"\n",
        "  )\n",
        "\n",
        "  conexion.commit()\n",
        "  conexion.close()\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Ocorreu um erro na tentativa de conexão: \", e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1kV0gLiGksb",
        "outputId": "f90e5134-f508-488b-8766-a32381ef5a26"
      },
      "source": [
        "try:\n",
        "  conexion = pyodbc.connect(DRIVER = '{ODBC Driver 17 for SQL Server}',\n",
        "                          SERVER = credentials['mssql_server'],\n",
        "                          DATABASE = credentials['mssql_database'],\n",
        "                          UID = credentials['mssql_uid'],\n",
        "                          PWD = credentials['mssql_pwd'])\n",
        "\n",
        "  cursor = conexion.cursor()\n",
        "\n",
        "  cursor.execute(\n",
        "      \"\"\"\n",
        "      SELECT * FROM INFORMATION_SCHEMA.TABLES;\n",
        "      \"\"\"\n",
        "      )\n",
        "\n",
        "  tables = cursor.fetchall()\n",
        "  conexion.close()\n",
        "\n",
        "  for table in tables:\n",
        "    print(table)\n",
        "\n",
        "\n",
        "    \n",
        "except Exception as e:\n",
        "  print(\"Ocorreu um erro na tentativa de conexão: \", e)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('olist_data', 'dbo', 'geolocation_dataset', 'BASE TABLE')\n",
            "('olist_data', 'dbo', 'order_items_dataset', 'BASE TABLE')\n",
            "('olist_data', 'dbo', 'order_payments_dataset', 'BASE TABLE')\n",
            "('olist_data', 'dbo', 'order_reviews_dataset', 'BASE TABLE')\n",
            "('olist_data', 'dbo', 'orders_dataset', 'BASE TABLE')\n",
            "('olist_data', 'dbo', 'products_dataset', 'BASE TABLE')\n",
            "('olist_data', 'dbo', 'sellers_dataset', 'BASE TABLE')\n",
            "('olist_data', 'dbo', 'product_category_name_translation', 'BASE TABLE')\n",
            "('olist_data', 'dbo', 'customers_dataset', 'BASE TABLE')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7s-BxaWSiXQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "614a47f2-69ea-4a0e-88ad-d11775ec4e25"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# olist_customers_dataset.csv\n",
        "customers_dataset_url = 'https://raw.githubusercontent.com/olist/work-at-olist-data/master/datasets/olist_customers_dataset.csv'\n",
        "customers_dataset = pd.read_csv(customers_dataset_url)\n",
        "\n",
        "# olist_geolocation_dataset.csv\n",
        "geolocation_dataset_url = 'https://raw.githubusercontent.com/olist/work-at-olist-data/master/datasets/olist_geolocation_dataset.csv'\n",
        "geolocation_dataset = pd.read_csv(geolocation_dataset_url)\n",
        "\n",
        "# olist_order_items_dataset.csv\n",
        "order_items_dataset_url = 'https://raw.githubusercontent.com/olist/work-at-olist-data/master/datasets/olist_order_items_dataset.csv'\n",
        "order_items_dataset = pd.read_csv(order_items_dataset_url)\n",
        "\n",
        "# olist_order_payments_dataset.csv\n",
        "order_payments_dataset_url = 'https://raw.githubusercontent.com/olist/work-at-olist-data/master/datasets/olist_order_payments_dataset.csv'\n",
        "order_payments_dataset = pd.read_csv(order_payments_dataset_url)\n",
        "\n",
        "# olist_order_reviews_dataset.csv\n",
        "order_reviews_dataset_url = 'https://raw.githubusercontent.com/olist/work-at-olist-data/master/datasets/olist_order_reviews_dataset.csv'\n",
        "order_reviews_dataset = pd.read_csv(order_reviews_dataset_url)\n",
        "\n",
        "# olist_orders_dataset.csv\n",
        "orders_dataset_url = 'https://raw.githubusercontent.com/olist/work-at-olist-data/master/datasets/olist_orders_dataset.csv'\n",
        "orders_dataset = pd.read_csv(orders_dataset_url)\n",
        "\n",
        "# olist_products_dataset.csv\n",
        "products_dataset_url = 'https://raw.githubusercontent.com/olist/work-at-olist-data/master/datasets/olist_products_dataset.csv'\n",
        "products_dataset = pd.read_csv(products_dataset_url)\n",
        "\n",
        "# olist_sellers_dataset.csv\n",
        "sellers_dataset_url = 'https://raw.githubusercontent.com/olist/work-at-olist-data/master/datasets/olist_sellers_dataset.csv'\n",
        "sellers_dataset = pd.read_csv(sellers_dataset_url)\n",
        "\n",
        "# product_category_name_translation.csv\n",
        "product_category_name_translation_url = 'https://raw.githubusercontent.com/olist/work-at-olist-data/master/datasets/product_category_name_translation.csv'\n",
        "product_category_name_translation = pd.read_csv(product_category_name_translation_url)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (10,11,12) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2AQ-RVC8-EO"
      },
      "source": [
        "def mssql_olist_data_bulk_insert_dataframe(dataframe, table_name):\r\n",
        "\r\n",
        "  import numpy as np\r\n",
        "  import urllib\r\n",
        "  import sqlalchemy\r\n",
        "  from datetime import datetime\r\n",
        "  import pandas as pd\r\n",
        "  import pyodbc \r\n",
        "\r\n",
        "  connection_string = \"DRIVER={};SERVER={};DATABASE={};UID={};PWD={}\".format('ODBC Driver 17 for SQL Server',\r\n",
        "                                                                             credentials['mssql_server'],\r\n",
        "                                                                             credentials['mssql_database'],\r\n",
        "                                                                             credentials['mssql_uid'],                                   \r\n",
        "                                                                             credentials['mssql_pwd'])\r\n",
        "\r\n",
        "  batch_size = 200000\r\n",
        "  batches = np.ceil(dataframe.shape[0] / batch_size)\r\n",
        "\r\n",
        "  for i in range(int(batches)):\r\n",
        "    \r\n",
        "    if i == batches - 1:\r\n",
        "      inicio = i * batch_size\r\n",
        "      final = dataframe.shape[0]\r\n",
        "    \r\n",
        "    else:\r\n",
        "      inicio = i * batch_size\r\n",
        "      final = inicio + batch_size\r\n",
        "\r\n",
        "    temp_dataframe = dataframe.iloc[inicio: final]\r\n",
        "\r\n",
        "    params = urllib.parse.quote_plus(connection_string)\r\n",
        "    engine = sqlalchemy.create_engine(\"mssql+pyodbc:///?odbc_connect=%s\" % params, fast_executemany=True)\r\n",
        "\r\n",
        "    temp_dataframe.to_sql(table_name, engine, if_exists='append', index=False)\r\n",
        "\r\n",
        "    # print(f'Batch: {i+1} de {int(batches)}')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTdTiyGLErOJ"
      },
      "source": [
        "def mssql_clear_table(table_name):\r\n",
        "\r\n",
        "  try:\r\n",
        "      conexion = pyodbc.connect(DRIVER = '{ODBC Driver 17 for SQL Server}',\r\n",
        "                                SERVER = credentials['mssql_server'],\r\n",
        "                                DATABASE = credentials['mssql_database'],\r\n",
        "                                UID = credentials['mssql_uid'],\r\n",
        "                                PWD = credentials['mssql_pwd'])\r\n",
        "      \r\n",
        "\r\n",
        "      cursor = conexion.cursor()\r\n",
        "      cursor.execute(f'delete from {table_name};')\r\n",
        "\r\n",
        "      conexion.commit()\r\n",
        "      conexion.close()\r\n",
        "\r\n",
        "      \r\n",
        "  except Exception as e:\r\n",
        "      print(\"Ocorreu um erro na tentativa de conexão: \", e) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9Pnb-lmGJav"
      },
      "source": [
        "# customers_dataset\r\n",
        "mssql_clear_table('customers_dataset')\r\n",
        "mssql_olist_data_bulk_insert_dataframe(customers_dataset, 'customers_dataset')\r\n",
        "\r\n",
        "# geolocation_dataset\r\n",
        "mssql_clear_table('geolocation_dataset')\r\n",
        "mssql_olist_data_bulk_insert_dataframe(geolocation_dataset, 'geolocation_dataset')\r\n",
        "\r\n",
        "# order_items_dataset\r\n",
        "mssql_clear_table('order_items_dataset')\r\n",
        "mssql_olist_data_bulk_insert_dataframe(order_items_dataset, 'order_items_dataset')\r\n",
        "\r\n",
        "# order_payments_dataset\r\n",
        "mssql_clear_table('order_payments_dataset')\r\n",
        "mssql_olist_data_bulk_insert_dataframe(order_payments_dataset, 'order_payments_dataset')\r\n",
        "\r\n",
        "# order_reviews_dataset\r\n",
        "mssql_clear_table('order_reviews_dataset')\r\n",
        "mssql_olist_data_bulk_insert_dataframe(order_reviews_dataset, 'order_reviews_dataset')\r\n",
        "\r\n",
        "# orders_dataset\r\n",
        "mssql_clear_table('orders_dataset')\r\n",
        "mssql_olist_data_bulk_insert_dataframe(orders_dataset, 'orders_dataset')\r\n",
        "\r\n",
        "# products_dataset\r\n",
        "mssql_clear_table('products_dataset')\r\n",
        "mssql_olist_data_bulk_insert_dataframe(products_dataset, 'products_dataset')\r\n",
        "\r\n",
        "# sellers_dataset\r\n",
        "mssql_clear_table('sellers_dataset')\r\n",
        "mssql_olist_data_bulk_insert_dataframe(sellers_dataset, 'sellers_dataset')\r\n",
        "\r\n",
        "# product_category_name_translation\r\n",
        "mssql_clear_table('product_category_name_translation')\r\n",
        "mssql_olist_data_bulk_insert_dataframe(product_category_name_translation, 'product_category_name_translation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gsw79xj_vIsg",
        "outputId": "a1d1f375-82b3-45a7-936c-2c956388bbf7"
      },
      "source": [
        "try:\r\n",
        "  conexion = pyodbc.connect(DRIVER = '{ODBC Driver 17 for SQL Server}',\r\n",
        "                            SERVER = credentials['mssql_server'],\r\n",
        "                            DATABASE = credentials['mssql_database'],\r\n",
        "                            UID = credentials['mssql_uid'],\r\n",
        "                            PWD = credentials['mssql_pwd'])\r\n",
        "\r\n",
        "\r\n",
        "  results = pd.read_sql('SELECT * FROM INFORMATION_SCHEMA.TABLES;', conexion)\r\n",
        "\r\n",
        "  for tabela in results['TABLE_NAME']:\r\n",
        "    # print(tabela)\r\n",
        "    print(tabela, pd.read_sql(f'SELECT * FROM {tabela};', conexion).shape[0])\r\n",
        "\r\n",
        "  conexion.close()\r\n",
        "   \r\n",
        "    \r\n",
        "except Exception as e:\r\n",
        "  print(\"Ocorreu um erro na tentativa de conexão: \", e)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "geolocation_dataset 1000163\n",
            "order_items_dataset 112650\n",
            "order_payments_dataset 103886\n",
            "order_reviews_dataset 100000\n",
            "orders_dataset 99441\n",
            "products_dataset 32951\n",
            "sellers_dataset 3095\n",
            "product_category_name_translation 71\n",
            "customers_dataset 99441\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}